{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c13a28-8864-408c-9dc9-e78518852405",
   "metadata": {},
   "source": [
    "## <center>**Requirements Multi-class Classification**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcab8e-416d-45bb-ac80-2fcf193c39a0",
   "metadata": {},
   "source": [
    "### **1. Construct the Task Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c455d0cf-05f1-49ff-bb19-c373463a7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.fileUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7fb682a4-71cf-40f7-87c0-731b0d10046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = readExcelToList('./promise_nfr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1f26cd4f-0129-4604-8f63-fef4c0bc5b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "21bd6de7-3325-4e74-a9b5-aa2561daf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lines = []\n",
    "for line in lines[1:]:\n",
    "    label = line[3].strip()\n",
    "    if label in ['PE','SE','US','O']:\n",
    "        data_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a6a3bf03-7ba0-4a3b-a9c9-2fa45abbb388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column 2 for Req text\n",
    "# column 3 for NFR sub class\n",
    "len(data_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e8341574-20fc-4b78-943f-c31b97642417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a41106b1-fa6c-4a7c-b051-e6e20a4941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'YOUR_OWN_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1b3266d8-0b73-44f1-8afe-7d0922e30b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only adjusters can request recycled parts audit reports. No users without an adjuster role shall request recycled parts audits. SE\n"
     ]
    }
   ],
   "source": [
    "test_req = data_lines[89]\n",
    "print(test_req[2],test_req[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "074454e7-dd6b-428c-809a-be0fe90f09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.\n",
    "prompt = '''Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.:\\n\\n\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3329f9f3-5432-4fea-b261-f8cc77c1e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt + test_req[2]}],\n",
    "        temperature = 0,\n",
    "        top_p = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "34dadb98-2951-4e2e-a95f-c2fd16b76b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29c17300-c72a-4035-a3eb-435508bba1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Security'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9bb8fccd-b14d-43c2-8f09-11ebc53f31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.:\n",
      "\n",
      "\n",
      "Only adjusters can request recycled parts audit reports. No users without an adjuster role shall request recycled parts audits.\n"
     ]
    }
   ],
   "source": [
    "print(prompt + test_req[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b13733-a9c5-4b3c-89e6-0aea33bdeb91",
   "metadata": {},
   "source": [
    "### **2. Multi-class Classification on Requirements with ChatGPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "058579fc-f41b-499d-bab9-2fae3d9aacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ChatGPT(req_str):\n",
    "    # Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.:\\n\\n\\n\n",
    "    prompt = 'Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.:\\n\\n\\n'\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt + req_str}],\n",
    "        temperature = 0,\n",
    "        top_p = 1\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    label = content.strip()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2f9bfa2e-7873-4a95-a547-84fc5d7d405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# current time\n",
    "now = datetime.now()\n",
    "# current time string\n",
    "time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename='./{}.log'.format(time_str),\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3a37ed18-8b7d-430b-8449-0084600776a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = readExcelToList('./promise_nfr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5a08a27b-15d2-4261-9165-6e72001d359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(lines) == (625+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f2ae72-a61e-418d-bc61-faa55245bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lines = []\n",
    "for line in lines[1:]:\n",
    "    label = line[3].strip()\n",
    "    if label in ['PE','SE','US','O']:\n",
    "        data_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "901de3e8-d212-4020-a1ee-ddcb4811c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_lines) == 249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1485508-da71-4cba-a120-df2888acadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_list = []\n",
    "pre_list = []\n",
    "for line in data_lines:\n",
    "    req_id = line[0].strip()\n",
    "    logging.info(req_id)\n",
    "    ans_label = line[3].strip()\n",
    "    ans_list.append(ans_label)\n",
    "    req_text = line[2].strip()\n",
    "    while True:\n",
    "        try:\n",
    "            ans_content = query_ChatGPT(req_text)\n",
    "            pre_list.append(ans_content)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27098541-43ff-4411-b60b-377a4263e3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(pre_list) == 249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f55f17c-0b91-4e26-bd29-26e2e16d07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(pre_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ad60721-f5b7-4c5c-a40c-9107440e3b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Operational',\n",
       " 'Operational.',\n",
       " 'Performance',\n",
       " 'Performance.',\n",
       " 'Security',\n",
       " 'Security.',\n",
       " 'Usability',\n",
       " 'Usability.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e094a6c-6fbb-4bac-81ea-76d034e38706",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_list_l = []\n",
    "for item in pre_list:\n",
    "    if item.startswith('O'):\n",
    "        pre_list_l.append('O')\n",
    "    else:\n",
    "        pre_list_l.append(item[:2].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c9eeffd5-9589-46ea-86af-5b67736c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumpJson('./pre_list_raw.json',pre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "50f96020-cda1-42e2-a624-21d50fc19b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumpJson('./pre_list_clean.json',pre_list_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66447738-85e2-4907-ab90-c653a2a7d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumpJson('./ans_list.json',ans_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c115c-1c93-466e-9ca3-461143b70d75",
   "metadata": {},
   "source": [
    "### **3. Leave-one-project-out cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9cce6-a93f-4287-9264-dcf2a8d0923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_list_l = loadJson('./pre_list_clean.json')\n",
    "# ans_list = loadJson('./ans_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bc667246-494b-446f-8e4b-e932bcbb9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have cleaned the ChatGPT responses with reference to the answer set, \n",
    "# set the postprocessed to True and continue to run the following evaluation codes.\n",
    "postprocessed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "75699c87-4b28-42df-95b7-5bb018dad627",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "See the following note!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30189/2418639239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mpostprocessed\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"See the following note!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: See the following note!"
     ]
    }
   ],
   "source": [
    "assert postprocessed == True, \"See the following note!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e1e8-62ce-4044-9c56-0ad4695f30e4",
   "metadata": {},
   "source": [
    "**Note**: Before running the following evaluation codes, you should firstly clean the ChatGPT responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8dd279ef-97f4-4c7c-b96a-2a2a6b881d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4fb9ba61-4f40-4b5a-a084-a94708a53404",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfr_labels = ['PE','SE','US','O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "997c3fe9-468b-40ee-9680-f220b079f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dict = {}\n",
    "for idx,line in enumerate(data_lines):\n",
    "    req_id = idx\n",
    "    proj_id = int(line[1].strip())\n",
    "    project_dict.setdefault(proj_id,[])\n",
    "    project_dict[proj_id].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5dca0261-8ea6-4c82-97e0-4f6199d2aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Num: 15\n"
     ]
    }
   ],
   "source": [
    "project_num = len(project_dict.keys())\n",
    "print('Project Num:', project_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f587bdb1-90b6-4a93-9462-0e808677894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of project_results is (# of projects, # of NFR subclass, three measures<P,R,F1>)\n",
    "project_results = np.zeros((project_num,len(nfr_labels),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "99a1d82a-de2e-464a-9ab3-a948781bcb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for proj_id in project_dict.keys():\n",
    "    proj_ans_list = []\n",
    "    proj_pre_list = []\n",
    "    for idx in project_dict[proj_id]:\n",
    "        ans = ans_list[idx]\n",
    "        pre = pre_list_l[idx]\n",
    "        proj_ans_list.append(ans)\n",
    "        proj_pre_list.append(pre)\n",
    "    report = precision_recall_fscore_support(proj_ans_list, proj_pre_list,labels=nfr_labels)\n",
    "    # In the transposed matrix, each line corresponds a NFR subclass.\n",
    "    # Columns correspond seperately to precision, recall, and F1 scores.\n",
    "    report_matrix = np.transpose(np.asarray(report))[:,:-1]\n",
    "    project_results[proj_id-1] = report_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3fe2b23f-0dbf-4bd2-8244-9766632c4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "14567315-b8b7-400b-b962-b7b94cef87b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.77948718, 12.78039216, 11.87893218],\n",
       "       [13.5       , 13.3       , 13.26797386],\n",
       "       [10.75728291, 11.31282051, 10.84047619],\n",
       "       [ 8.53418803,  8.29206349,  8.13492063]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the measures of each NFR sub-class over 15 projects\n",
    "np.sum(project_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "35f22172-c0ff-453f-9610-d0153c1fb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False  True]\n",
      " [False False False  True]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " [False False  True False]\n",
      " [False False False False]\n",
      " [False  True  True False]\n",
      " [False False False  True]\n",
      " [False False False  True]\n",
      " [False False False False]\n",
      " [ True False False False]\n",
      " [False False False False]\n",
      " [False False False False]]\n"
     ]
    }
   ],
   "source": [
    "# Find the all zeros rows. That means some NFR subclass does not appear in a project\n",
    "all_zeros = np.all(project_results == 0, axis=2)\n",
    "# e.g. the element in (1,3) is True. That means the Operational(O) NFR requirements do not appear in Project 2.\n",
    "print(all_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "648881e3-a06f-4b85-a172-3d7be37c64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  6,  8,  8,  9, 10, 12]), array([3, 3, 2, 1, 2, 3, 3, 0]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first arr includes project IDs (starts from 0). \n",
    "# The second arr includes the index of NFR sub-classes, nfr_labels = ['PE','SE','US','O'] \n",
    "np.where(all_zeros==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b65c3-8ecc-4226-ae8a-2f6c6afffff6",
   "metadata": {},
   "source": [
    "#### **3.1 Average performance of PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d6c9d013-b52f-42d0-831d-84d2fc0c83e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# compute the number of projects which do not include the PE requirements\n",
    "label_idx = nfr_labels.index('PE')\n",
    "PE_abs_num = np.count_nonzero(np.where(all_zeros==True)[1]==label_idx)\n",
    "print(PE_abs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b27d3440-30b2-4dcf-bc44-fac624c029b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84, 0.91, 0.85])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the average performance measures of PE over projects which include that type of NFR.\n",
    "np.around(\n",
    "    np.sum(project_results,axis=0)[label_idx]/(project_num-PE_abs_num),\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eebc8c-9a8b-409e-b966-b6e0b928a7da",
   "metadata": {},
   "source": [
    "#### **3.2 Average performance of SE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a6ee884a-feb1-4be6-988a-02119fbe0afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# compute the number of projects which do not include the SE requirements\n",
    "label_idx = nfr_labels.index('SE')\n",
    "SE_abs_num = np.count_nonzero(np.where(all_zeros==True)[1]==label_idx)\n",
    "print(SE_abs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bad2d978-9172-4dfa-ad75-6e9c0c7ecbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96, 0.95, 0.95])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the average performance measures of SE over projects which include that type of NFR.\n",
    "np.around(\n",
    "    np.sum(project_results,axis=0)[label_idx]/(project_num-SE_abs_num),\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8a23d-f00b-4f97-9c3a-f41d7e3eb6c6",
   "metadata": {},
   "source": [
    "#### **3.3 Average performance of US**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0e338c0d-237f-4b41-8166-392f2d197d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# compute the number of projects which do not include the US requirements\n",
    "label_idx = nfr_labels.index('US')\n",
    "US_abs_num = np.count_nonzero(np.where(all_zeros==True)[1]==label_idx)\n",
    "print(US_abs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "83b6a457-89d4-4e6d-83de-67ec10c44ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83, 0.87, 0.83])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the average performance measures of US over projects which include that type of NFR.\n",
    "np.around(\n",
    "    np.sum(project_results,axis=0)[label_idx]/(project_num-US_abs_num),\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a342-eb69-4373-bf17-d1449da2892c",
   "metadata": {},
   "source": [
    "#### **3.4 Average performance of O**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a3a69c26-4547-4f74-914e-b245677a2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# compute the number of projects which do not include the O requirements\n",
    "label_idx = nfr_labels.index('O')\n",
    "O_abs_num = np.count_nonzero(np.where(all_zeros==True)[1]==label_idx)\n",
    "print(O_abs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "476cf38b-399f-4a8b-b719-c39d43285885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78, 0.75, 0.74])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the average performance measures of US over projects which include that type of NFR.\n",
    "np.around(\n",
    "    np.sum(project_results,axis=0)[label_idx]/(project_num-O_abs_num),\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc8ab5-3138-4d68-b7e8-e28d442d9262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
